{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceLimit=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define scalers for velocity and timing features\n",
    "velocity_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "time_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract MIDI features\n",
    "def extract_midi_features(midi_file):\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    notes = []  # Initialize notes inside the function\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append([\n",
    "                note.pitch, \n",
    "                note.velocity,\n",
    "                note.start, \n",
    "                note.end \n",
    "            ])\n",
    "    \n",
    "    return np.array(notes)  # Return all notes\n",
    "\n",
    "# Load all MIDI files\n",
    "midi_files = glob.glob(\"smaller-dataset/*.midi\")\n",
    "data = []\n",
    "for f in midi_files:\n",
    "    notes = extract_midi_features(f)\n",
    "    for i in range(0, len(notes) - 15):\n",
    "        data.append(notes[i:i+16])  # Create 16-note sequences\n",
    "data = [d for d in data if d.shape == (16, 4)]  # Filter inconsistent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16, 4])\n",
      "torch.Size([100, 16, 4])\n",
      "tensor([[74.0000, 92.0000,  1.0234,  1.0859],\n",
      "        [57.0000, 79.0000,  2.0312,  2.0951],\n",
      "        [62.0000, 86.0000,  2.5339,  2.5768],\n",
      "        [81.0000, 93.0000,  1.5456,  2.5990],\n",
      "        [74.0000, 82.0000,  3.0247,  3.4714],\n",
      "        [78.0000, 97.0000,  3.0156,  3.5299],\n",
      "        [73.0000, 73.0000,  3.5182,  3.5625],\n",
      "        [76.0000, 79.0000,  3.5273,  3.5768],\n",
      "        [71.0000, 78.0000,  3.6615,  3.7005],\n",
      "        [74.0000, 72.0000,  3.6667,  3.7148],\n",
      "        [69.0000, 76.0000,  3.7969,  3.8281],\n",
      "        [73.0000, 79.0000,  3.7852,  3.8568],\n",
      "        [67.0000, 78.0000,  3.9284,  3.9661],\n",
      "        [71.0000, 83.0000,  3.9271,  3.9844],\n",
      "        [66.0000, 76.0000,  4.0638,  4.0990],\n",
      "        [69.0000, 77.0000,  4.0625,  4.1133]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "data = data[:sequenceLimit]\n",
    "dataset = torch.tensor(np.array(data), dtype=torch.float32)\n",
    "print(dataset.shape)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch training\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Transformer Model with Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=16):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe.to(x.device)\n",
    "\n",
    "class MidiTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=4, model_dim=128, num_heads=4, num_layers=3, ff_dim=512):\n",
    "        super(MidiTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, nhead=num_heads, dim_feedforward=ff_dim\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(model_dim, input_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = self.pos_encoder(x)  \n",
    "        x = self.transformer_encoder(x)  \n",
    "        x = self.fc(x)  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2585.9283447265625\n",
      "Epoch 10, Loss: 2097.56591796875\n",
      "Epoch 20, Loss: 1668.2442321777344\n",
      "Epoch 30, Loss: 1255.4702453613281\n",
      "Epoch 40, Loss: 819.5365447998047\n",
      "Epoch 50, Loss: 525.4490509033203\n",
      "Epoch 60, Loss: 282.96331787109375\n",
      "Epoch 70, Loss: 148.54865646362305\n",
      "Epoch 80, Loss: 67.87928676605225\n",
      "Epoch 90, Loss: 43.35159683227539\n"
     ]
    }
   ],
   "source": [
    "# Model, loss function, and optimizer\n",
    "model = MidiTransformer()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with batches\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)  \n",
    "        loss = criterion(outputs, batch)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0836, -0.4635,  0.2884,  1.1038],\n",
      "         [-1.6744, -1.4108,  0.7726, -1.4022],\n",
      "         [-1.2442, -0.4962, -0.4393,  0.1786],\n",
      "         [ 0.4780,  1.1954, -0.6019, -1.0079],\n",
      "         [-0.5620, -0.3752,  0.7985, -0.4743],\n",
      "         [-1.0315, -0.9461,  0.1202,  0.9997],\n",
      "         [ 0.3500,  0.5563, -0.9470,  1.1756],\n",
      "         [ 0.2446,  1.6437, -0.0086,  1.3778],\n",
      "         [-2.4257,  0.0209, -0.0960, -0.8565],\n",
      "         [-1.8921,  0.4859, -0.8509, -0.0085],\n",
      "         [-0.3115,  0.5166,  0.2960,  0.7540],\n",
      "         [ 1.0420, -1.5239, -1.3521,  1.5331],\n",
      "         [-0.0267,  0.3463, -1.7751, -0.6858],\n",
      "         [ 0.5072, -0.0839, -0.5785, -1.0536],\n",
      "         [-0.0783,  2.1100, -0.2625,  1.3125],\n",
      "         [-1.8182,  1.2010, -0.4186, -0.6343]]])\n",
      "difference\n",
      "tensor([[[81.0000, 93.0000,  1.5456,  2.5990],\n",
      "         [74.0000, 82.0000,  3.0247,  3.4714],\n",
      "         [78.0000, 97.0000,  3.0156,  3.5299],\n",
      "         [73.0000, 73.0000,  3.5182,  3.5625],\n",
      "         [76.0000, 79.0000,  3.5273,  3.5768],\n",
      "         [71.0000, 78.0000,  3.6615,  3.7005],\n",
      "         [74.0000, 72.0000,  3.6667,  3.7148],\n",
      "         [69.0000, 76.0000,  3.7969,  3.8281],\n",
      "         [73.0000, 79.0000,  3.7852,  3.8568],\n",
      "         [67.0000, 78.0000,  3.9284,  3.9661],\n",
      "         [71.0000, 83.0000,  3.9271,  3.9844],\n",
      "         [66.0000, 76.0000,  4.0638,  4.0990],\n",
      "         [69.0000, 77.0000,  4.0625,  4.1133],\n",
      "         [64.0000, 73.0000,  4.1953,  4.2435],\n",
      "         [67.0000, 72.0000,  4.1953,  4.2721],\n",
      "         [62.0000, 80.0000,  4.3581,  4.3854]]])\n"
     ]
    }
   ],
   "source": [
    "# Generate a new MIDI sequence from a random input\n",
    "\n",
    "random_sequence_rangen = torch.randn(1, 16, 4) \n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "random_sequence = dataset[random_index].unsqueeze(0)\n",
    "\n",
    "print(random_sequence_rangen)\n",
    "print('difference')\n",
    "print(random_sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[69.20963  , 72.29453  ,  7.052752 ,  7.263224 ],\n",
       "        [69.179214 , 72.22012  ,  7.246605 ,  7.5992837],\n",
       "        [68.3742   , 72.14647  ,  6.9332657,  7.090119 ],\n",
       "        [69.80344  , 71.97262  ,  7.5167904,  7.6648884],\n",
       "        [69.0667   , 72.15718  ,  7.045172 ,  7.223279 ],\n",
       "        [69.10411  , 72.18678  ,  7.329755 ,  7.458869 ],\n",
       "        [70.00176  , 71.9509   ,  7.459717 ,  7.735195 ],\n",
       "        [68.80994  , 72.23756  ,  6.9713926,  7.217267 ],\n",
       "        [69.3366   , 72.33603  ,  7.3750324,  7.54369  ],\n",
       "        [68.68713  , 72.143814 ,  7.1835613,  7.3560743],\n",
       "        [68.794395 , 72.33178  ,  7.194614 ,  7.3136854],\n",
       "        [68.68133  , 72.24837  ,  7.1595764,  7.2297783],\n",
       "        [68.85937  , 72.2536   ,  7.0949516,  7.151725 ],\n",
       "        [69.3394   , 72.37222  ,  7.34568  ,  7.423238 ],\n",
       "        [69.14375  , 72.13284  ,  7.2052336,  7.2926397],\n",
       "        [67.4187   , 72.11232  ,  6.746956 ,  6.7287765]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequence = model(random_sequence).detach().numpy()\n",
    "generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert generated sequence to MIDI\n",
    "def sequence_to_midi(sequence, output_file=\"generated.mid\"):\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=0)  \n",
    "\n",
    "    for note_data in sequence:\n",
    "        pitch, velocity, start, duration = map(int, note_data)\n",
    "        end = start + duration\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=max(0, min(127, velocity)),  \n",
    "            pitch=max(0, min(127, pitch)),  \n",
    "            start=max(0, start),\n",
    "            end=max(0, end)\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "\n",
    "    midi.instruments.append(instrument)\n",
    "    midi.write(output_file)\n",
    "    print(f\"Generated MIDI saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated MIDI saved as generated.mid\n"
     ]
    }
   ],
   "source": [
    "sequence_to_midi(generated_sequence[0], \"generated.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
